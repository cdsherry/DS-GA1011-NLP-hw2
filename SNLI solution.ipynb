{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load snli train and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load snli train data\n",
    "snli_train = pd.read_csv(\"snli_train.tsv\", sep='\\t')\n",
    "snli_train.loc[snli_train['label'] == 'neutral', 'y'] = 0\n",
    "snli_train.loc[snli_train['label'] == 'entailment', 'y'] = 1\n",
    "snli_train.loc[snli_train['label'] == 'contradiction', 'y'] = 2\n",
    "train_s1 = snli_train['sentence1'].tolist()\n",
    "train_s2 = snli_train['sentence2'].tolist()\n",
    "train_label = snli_train['y'].tolist()\n",
    "\n",
    "# load snli val data\n",
    "snli_val = pd.read_csv(\"snli_val.tsv\", sep='\\t')\n",
    "snli_val.loc[snli_val['label'] == 'neutral', 'y'] = 0\n",
    "snli_val.loc[snli_val['label'] == 'entailment', 'y'] = 1\n",
    "snli_val.loc[snli_val['label'] == 'contradiction', 'y'] = 2\n",
    "val_s1 = snli_val['sentence1'].tolist()\n",
    "val_s2 = snli_val['sentence2'].tolist()\n",
    "val_label = snli_val['y'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the FastText embedding matrix and get the indices data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Fast text vectors\n",
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings = np.zeros((words_to_load + 1, 300))\n",
    "    words2idx = {}\n",
    "    idx2words = {}\n",
    "    ordered_words = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings[i + 1, :] = np.asarray(s[1:])\n",
    "        words2idx[s[0]] = i + 1\n",
    "        idx2words[i+1] = s[0]\n",
    "        ordered_words.append(s[0])\n",
    "        \n",
    "idx2words[PAD_IDX] = '<pad>'\n",
    "idx2words[UNK_IDX] = '<unk>'\n",
    "words2idx['<pad>'] = PAD_IDX\n",
    "words2idx['<unk>'] = UNK_IDX\n",
    "# make a random vector for the unknown, and a zero vector for the padding\n",
    "loaded_embeddings[PAD_IDX] = torch.zeros(300)\n",
    "loaded_embeddings[UNK_IDX] = torch.randn(300)\n",
    "pre_emb = torch.from_numpy(loaded_embeddings)\n",
    "pre_emb = pre_emb.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_dataset):\n",
    "    indices_data = []\n",
    "    for tokens_data in tokens_dataset:\n",
    "        tokens = tokens_data.split()\n",
    "        index_list = [words2idx[token] if token in words2idx else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_s1_idx = token2index_dataset(train_s1)\n",
    "train_s2_idx = token2index_dataset(train_s2)\n",
    "val_s1_idx = token2index_dataset(val_s1)\n",
    "val_s2_idx = token2index_dataset(val_s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data with data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = max([len(s) for s in train_s1_idx + train_s2_idx])\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of tokens\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        # s1, s2: sentence1 and sentence2\n",
    "        # l1, l2: length of s1 and s2\n",
    "        s1 = self.data_list[key][0][:MAX_SENTENCE_LENGTH]\n",
    "        s2 = self.data_list[key][1][:MAX_SENTENCE_LENGTH]\n",
    "        l1 = len(s1)\n",
    "        l2 = len(s2)\n",
    "        label = self.target_list[key]\n",
    "        return [[s1, s2], [l1, l2], label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding s1 and s2 separately\n",
    "    for datum in batch:\n",
    "        padded_s1 = np.pad(np.array(datum[0][0]),\n",
    "                           pad_width=((0,MAX_SENTENCE_LENGTH-datum[1][0])),\n",
    "                           mode=\"constant\", constant_values=0)\n",
    "        padded_s2 = np.pad(np.array(datum[0][1]),\n",
    "                           pad_width=((0,MAX_SENTENCE_LENGTH-datum[1][1])),\n",
    "                           mode=\"constant\", constant_values=0)\n",
    "        padded_vec = [padded_s1, padded_s2]\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the data with dataloader\n",
    "train_data = np.array([train_s1_idx, train_s2_idx]).transpose()\n",
    "val_data = np.array([val_s1_idx, val_s2_idx]).transpose()\n",
    "\n",
    "train_dataset = VocabDataset(train_data, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=vocab_collate_func, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_data, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         collate_fn=vocab_collate_func, \n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the functions to train and test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for (data, lengths, labels) in loader:\n",
    "        \n",
    "        outputs = F.softmax(model(data, lengths), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(train_loader, val_loader, model):\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, length, label) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data, length)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Training Acc: {}, Validation Acc: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc, val_acc))\n",
    "                train_acc_list.append(train_acc)\n",
    "                val_acc_list.append(val_acc)\n",
    "    return train_acc_list, val_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the data with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # Use pre-trained embedding matrix\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight = nn.Parameter(pre_emb)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # 2 layer 1-D convolutional network\n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        # use a max-pool at the end to compress the hidden representation into a single vector\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=MAX_SENTENCE_LENGTH)\n",
    "        \n",
    "        # 2 fully-connected layers\n",
    "        self.fc1 = nn.Linear(2 * hidden_size, hidden_size)        \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, data, length):\n",
    "        data = data.transpose(0,1)\n",
    "        \n",
    "        # first process s1\n",
    "        embed1 = self.embedding(data[0]).transpose(1,2)\n",
    "        s1 = self.conv1(embed1)\n",
    "        s1 = F.relu(s1.contiguous())\n",
    "        s1 = self.conv2(s1)\n",
    "        s1 = F.relu(s1.contiguous())\n",
    "        s1 = self.maxpool(s1)\n",
    "        \n",
    "        # then process s2\n",
    "        embed2 = self.embedding(data[1]).transpose(1,2)\n",
    "        s2 = self.conv1(embed2)\n",
    "        s2 = F.relu(s2.contiguous())\n",
    "        s2 = self.conv2(s2)\n",
    "        s2 = F.relu(s2.contiguous())\n",
    "        s2 = self.maxpool(s2)\n",
    "        \n",
    "        # simply concat s1 and s2, and feed it through a network of 2 fully-connected layers\n",
    "        x = torch.cat((s1,s2), dim=1).view(-1, 2 * self.hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CNN(emb_size=300, hidden_size=200, num_classes=3, vocab_size=len(idx2words))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "train_acc1, val_acc1 = train_model(train_loader, val_loader, model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the CNN model with smaller hidden size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CNN(emb_size=300, hidden_size=100, num_classes=3, vocab_size=len(idx2words))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "train_acc2, val_acc2 = train_model(train_loader, val_loader, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the CNN model so that two encoded sentences are interacted by element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        # Use pre-trained embedding matrix\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight = nn.Parameter(pre_emb)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=MAX_SENTENCE_LENGTH)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)        \n",
    "        self.fc2 = nn.Linear(hidden_size // 2, num_classes)\n",
    "\n",
    "    def forward(self, data, length):                \n",
    "        data = data.transpose(0,1)\n",
    "        \n",
    "        embed1 = self.embedding(data[0]).transpose(1,2)        \n",
    "        s1 = self.conv1(embed1)\n",
    "        s1 = F.relu(s1.contiguous())        \n",
    "        s1 = self.conv2(s1)\n",
    "        s1 = F.relu(s1.contiguous())        \n",
    "        s1 = self.maxpool(s1)\n",
    "        \n",
    "        embed2 = self.embedding(data[1]).transpose(1,2)        \n",
    "        s2 = self.conv1(embed2)\n",
    "        s2 = F.relu(s2.contiguous())\n",
    "        s2 = self.conv2(s2)\n",
    "        s2 = F.relu(s2.contiguous())        \n",
    "        s2 = self.maxpool(s2)\n",
    "        \n",
    "        # interacted s1 and s2 by element-wise multiplication\n",
    "        x = s1 * s2\n",
    "        x = x.view(-1, self.hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = CNN2(emb_size=300, hidden_size=200, num_classes=3, vocab_size=len(idx2words))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "\n",
    "train_acc3, val_acc3 = train_model(train_loader, val_loader, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of parameters in each CNN model\n",
    "print(\"numer of parameters in model 1 is {}\". format(sum(p.numel() for p in model1.parameters() if p.requires_grad)))\n",
    "print(\"numer of parameters in model 2 is {}\". format(sum(p.numel() for p in model2.parameters() if p.requires_grad)))\n",
    "print(\"numer of parameters in model 3 is {}\". format(sum(p.numel() for p in model3.parameters() if p.requires_grad)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.plot(train_acc1, label=\"model 1\")\n",
    "plt.plot(train_acc2, label=\"model 2\")\n",
    "plt.plot(train_acc3, label=\"model 3\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"train accuracy\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the validation accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.plot(val_acc1, label=\"model 1\")\n",
    "plt.plot(val_acc2, label=\"model 2\")\n",
    "plt.plot(val_acc3, label=\"model 3\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"validation accuracy\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 3 incorrect predictions and 3 correct predictions\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "val_dataset2 = VocabDataset(val_data, val_label)\n",
    "val_loader2 = torch.utils.data.DataLoader(dataset=val_dataset2, \n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         collate_fn=vocab_collate_func, \n",
    "                                         shuffle=True)\n",
    "\n",
    "y2label = {0:'neutral', 1:'entailment', 2:'contradiction'}\n",
    "\n",
    "def get_incorrect(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    incorr_data = []\n",
    "    for data, lengths, labels in loader:\n",
    "        if i < 3:\n",
    "            outputs = F.softmax(model(data, lengths), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            \n",
    "            if predicted.eq(labels.view_as(predicted)).sum().item() == 0:                \n",
    "                i += 1\n",
    "                for t in data[0]:\n",
    "                    s = ''\n",
    "                    for w in t:\n",
    "                        if w == 0: break\n",
    "                        w = w.item()\n",
    "                        s += idx2words[w] + ' '\n",
    "                    print(s)\n",
    "                print(\"The predicted label is {}\".format(y2label[predicted.item()]))\n",
    "                print(\"The true label is {}\".format(y2label[labels.view_as(predicted).item()]))\n",
    "                print()\n",
    "    return\n",
    "\n",
    "print(\"Get 3 incorrect predictions: \")\n",
    "get_incorrect(val_loader2, model3)\n",
    "\n",
    "def get_correct(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    incorr_data = []\n",
    "    for data, lengths, labels in loader:\n",
    "        if i < 3:\n",
    "            outputs = F.softmax(model(data, lengths), dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            \n",
    "            if predicted.eq(labels.view_as(predicted)).sum().item() == 1:                \n",
    "                i += 1\n",
    "                for t in data[0]:\n",
    "                    s = ''\n",
    "                    for w in t:\n",
    "                        if w == 0: break\n",
    "                        w = w.item()\n",
    "                        s += idx2words[w] + ' '\n",
    "                    print(s)\n",
    "                print(\"The label is {}\".format(y2label[predicted.item()]))\n",
    "                print()\n",
    "    return\n",
    "\n",
    "print(\"Get 3 correct predictions: \")\n",
    "get_correct(val_loader2, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the data with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight = nn.Parameter(pre_emb)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # use a single-layer, bi-directional GRU\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # the input is 4 * hidden_size since bi-directional has doubled the hidden size,\n",
    "        # and the concat of two sentences again doubled the hidden size\n",
    "        self.fc1 = nn.Linear(4 * hidden_size, 2 * hidden_size)\n",
    "        self.fc2 = nn.Linear(2 * hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2 * self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, data, lengths):\n",
    "        s1 = data.transpose(0,1)[0]\n",
    "        s2 = data.transpose(0,1)[1]        \n",
    "        l1 = lengths.transpose(0,1)[0]  \n",
    "        l2 = lengths.transpose(0,1)[1]  \n",
    "        \n",
    "        # sort two lists of sentences according to the length in descending order\n",
    "        l1, arg1 = l1.sort(descending=True)\n",
    "        l2, arg2 = l2.sort(descending=True)        \n",
    "        s1 = s1[arg1]\n",
    "        s2 = s2[arg2]\n",
    "        \n",
    "        batch_size, seq_len = s1.size()\n",
    "        \n",
    "        # process s1\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed1 = self.embedding(s1)\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, l1.numpy(), batch_first=True)\n",
    "        rnn_out1, self.hidden = self.gru(embed1, self.hidden)      \n",
    "        rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
    "        rnn_out1 = torch.sum(rnn_out1, dim=1)\n",
    "        # sort the encoded sentences back to the original order\n",
    "        arg1 = np.argsort(arg1)\n",
    "        rnn_out1 = rnn_out1[arg1]\n",
    "        \n",
    "        # process s2\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed2 = self.embedding(s2)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, l2.numpy(), batch_first=True)\n",
    "        rnn_out2, self.hidden = self.gru(embed2, self.hidden)\n",
    "        rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
    "        rnn_out2 = torch.sum(rnn_out2, dim=1)\n",
    "        # sort the encoded sentences back to the original order\n",
    "        arg2 = np.argsort(arg2)\n",
    "        rnn_out2 = rnn_out2[arg2]\n",
    "        \n",
    "        # simply concat s1 and s2, and feed it through a network of 2 fully-connected layers\n",
    "        rnn_out = torch.cat((rnn_out1, rnn_out2), dim = 1)\n",
    "        rnn_out = self.fc1(rnn_out)\n",
    "        rnn_out = self.fc2(rnn_out)\n",
    "        return rnn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = RNN(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, vocab_size=len(idx2words))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=learning_rate)\n",
    "\n",
    "train_acc4, val_acc4 = train_model(train_loader, val_loader, model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the RNN model with smaller hidden size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = RNN(emb_size=300, hidden_size=100, num_layers=1, num_classes=3, vocab_size=len(idx2words))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model5.parameters(), lr=learning_rate)\n",
    "\n",
    "train_acc5, val_acc5 = train_model(train_loader, val_loader, model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the RNN model so that two encoded sentences are interacted by element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN2(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN2, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight = nn.Parameter(pre_emb)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2 * self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, data, lengths):        \n",
    "        s1 = data.transpose(0,1)[0]\n",
    "        s2 = data.transpose(0,1)[1]        \n",
    "        l1 = lengths.transpose(0,1)[0]  \n",
    "        l2 = lengths.transpose(0,1)[1]  \n",
    "        \n",
    "        l1, arg1 = l1.sort(descending=True)\n",
    "        l2, arg2 = l2.sort(descending=True)        \n",
    "        s1 = s1[arg1]\n",
    "        s2 = s2[arg2]\n",
    "        \n",
    "        batch_size, seq_len = s1.size()\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed1 = self.embedding(s1)\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, l1.numpy(), batch_first=True)\n",
    "        rnn_out1, self.hidden = self.gru(embed1, self.hidden)      \n",
    "        rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
    "        rnn_out1 = torch.sum(rnn_out1, dim=1)\n",
    "        arg1 = np.argsort(arg1)\n",
    "        rnn_out1 = rnn_out1[arg1]\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed2 = self.embedding(s2)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, l2.numpy(), batch_first=True)\n",
    "        rnn_out2, self.hidden = self.gru(embed2, self.hidden)\n",
    "        rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
    "        rnn_out2 = torch.sum(rnn_out2, dim=1)\n",
    "        arg2 = np.argsort(arg2)\n",
    "        rnn_out2 = rnn_out2[arg2]\n",
    "        \n",
    "        # interacted the two encoded sequences by element-wise multiplication\n",
    "        rnn_out = rnn_out1 * rnn_out2\n",
    "        \n",
    "        rnn_out = self.fc1(rnn_out)\n",
    "        rnn_out = self.fc2(rnn_out)\n",
    "        return rnn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = RNN2(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, vocab_size=len(idx2words))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model6.parameters(), lr=learning_rate)\n",
    "\n",
    "train_acc6, val_acc6 = train_model(train_loader, val_loader, model6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the RNN model so we have additional regularization (dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN3(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN3, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight = nn.Parameter(pre_emb)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2 * self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, data, lengths):        \n",
    "        s1 = data.transpose(0,1)[0]\n",
    "        s2 = data.transpose(0,1)[1]        \n",
    "        l1 = lengths.transpose(0,1)[0]  \n",
    "        l2 = lengths.transpose(0,1)[1]  \n",
    "        \n",
    "        l1, arg1 = l1.sort(descending=True)\n",
    "        l2, arg2 = l2.sort(descending=True)        \n",
    "        s1 = s1[arg1]\n",
    "        s2 = s2[arg2]\n",
    "        \n",
    "        batch_size, seq_len = s1.size()\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed1 = self.embedding(s1)\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, l1.numpy(), batch_first=True)\n",
    "        rnn_out1, self.hidden = self.gru(embed1, self.hidden)        \n",
    "        rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
    "        # add the dropout step for s1\n",
    "        rnn_out1 = self.dropout(rnn_out1)\n",
    "        rnn_out1 = torch.sum(rnn_out1, dim=1)\n",
    "        arg1 = np.argsort(arg1)\n",
    "        rnn_out1 = rnn_out1[arg1]\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed2 = self.embedding(s2)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, l2.numpy(), batch_first=True)\n",
    "        rnn_out2, self.hidden = self.gru(embed2, self.hidden)\n",
    "        rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
    "        # add the dropout step for s2\n",
    "        rnn_out2 = self.dropout(rnn_out2)\n",
    "        rnn_out2 = torch.sum(rnn_out2, dim=1)\n",
    "        arg2 = np.argsort(arg2)\n",
    "        rnn_out2 = rnn_out2[arg2]\n",
    "        \n",
    "        rnn_out = rnn_out1 * rnn_out2\n",
    "        \n",
    "        rnn_out = self.fc1(rnn_out)\n",
    "        rnn_out = self.fc2(rnn_out)\n",
    "        return rnn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = RNN3(emb_size=300, hidden_size=200, num_layers=1, num_classes=3, vocab_size=len(idx2words))\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model7.parameters(), lr=learning_rate)\n",
    "\n",
    "train_acc7, val_acc7 = train_model(train_loader, val_loader, model7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of parameters in each CNN model\n",
    "print(\"numer of parameters in model 4 is {}\". format(sum(p.numel() for p in model4.parameters() if p.requires_grad)))\n",
    "print(\"numer of parameters in model 5 is {}\". format(sum(p.numel() for p in model5.parameters() if p.requires_grad)))\n",
    "print(\"numer of parameters in model 6 is {}\". format(sum(p.numel() for p in model6.parameters() if p.requires_grad)))\n",
    "print(\"numer of parameters in model 7 is {}\". format(sum(p.numel() for p in model7.parameters() if p.requires_grad)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.plot(train_acc4, label=\"model 4\")\n",
    "plt.plot(train_acc5, label=\"model 5\")\n",
    "plt.plot(train_acc6, label=\"model 6\")\n",
    "plt.plot(train_acc7, label=\"model 7\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"train accuracy\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.plot(val_acc4, label=\"model 4\")\n",
    "plt.plot(val_acc5, label=\"model 5\")\n",
    "plt.plot(val_acc6, label=\"model 6\")\n",
    "plt.plot(val_acc7, label=\"model 7\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"validation accuracy\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Get 3 incorrect predictions: \")\n",
    "get_incorrect(val_loader2, model3)\n",
    "print(\"Get 3 incorrect predictions: \")\n",
    "get_correct(val_loader2, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with mnli data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnli val data\n",
    "mnli_val = pd.read_csv(\"mnli_val.tsv\", sep='\\t')\n",
    "mnli_val.loc[mnli_val['label'] == 'neutral', 'y'] = 0\n",
    "mnli_val.loc[mnli_val['label'] == 'entailment', 'y'] = 1\n",
    "mnli_val.loc[mnli_val['label'] == 'contradiction', 'y'] = 2\n",
    "mnli_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the genres in the dataset\n",
    "mnli_val['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each genre, generate a specific dataloader for accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "government = mnli_val.loc[mnli_val['genre'] == 'government']\n",
    "gov_data = np.array([token2index_dataset(government['sentence1'].tolist()), token2index_dataset(government['sentence2'].tolist())]).transpose()\n",
    "gov_label = government['y'].tolist()\n",
    "gov_dataset = VocabDataset(gov_data, gov_label)\n",
    "gov_loader = torch.utils.data.DataLoader(dataset=gov_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=vocab_collate_func, \n",
    "                                           shuffle=True)\n",
    "print(test_model(gov_loader, model3))\n",
    "print(test_model(gov_loader, model6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telephone = mnli_val.loc[mnli_val['genre'] == 'telephone']\n",
    "tel_data = np.array([token2index_dataset(telephone['sentence1'].tolist()), token2index_dataset(telephone['sentence2'].tolist())]).transpose()\n",
    "tel_label = telephone['y'].tolist()\n",
    "tel_dataset = VocabDataset(tel_data, tel_label)\n",
    "tel_loader = torch.utils.data.DataLoader(dataset=tel_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=vocab_collate_func, \n",
    "                                           shuffle=True)\n",
    "print(test_model(tel_loader, model3))\n",
    "print(test_model(tel_loader, model6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate = mnli_val.loc[mnli_val['genre'] == 'slate']\n",
    "slate_data = np.array([token2index_dataset(slate['sentence1'].tolist()), token2index_dataset(slate['sentence2'].tolist())]).transpose()\n",
    "slate_label = slate['y'].tolist()\n",
    "slate_dataset = VocabDataset(slate_data, slate_label)\n",
    "slate_loader = torch.utils.data.DataLoader(dataset=slate_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=vocab_collate_func, \n",
    "                                           shuffle=True)\n",
    "print(test_model(slate_loader, model3))\n",
    "print(test_model(slate_loader, model6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction = mnli_val.loc[mnli_val['genre'] == 'fiction']\n",
    "fic_data = np.array([token2index_dataset(fiction['sentence1'].tolist()), token2index_dataset(fiction['sentence2'].tolist())]).transpose()\n",
    "fic_label = fiction['y'].tolist()\n",
    "fic_dataset = VocabDataset(fic_data, fic_label)\n",
    "fic_loader = torch.utils.data.DataLoader(dataset=fic_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=vocab_collate_func, \n",
    "                                           shuffle=True)\n",
    "print(test_model(fic_loader, model3))\n",
    "print(test_model(fic_loader, model6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel = mnli_val.loc[mnli_val['genre'] == 'travel']\n",
    "tvl_data = np.array([token2index_dataset(travel['sentence1'].tolist()), token2index_dataset(travel['sentence2'].tolist())]).transpose()\n",
    "tvl_label = travel['y'].tolist()\n",
    "tvl_dataset = VocabDataset(tvl_data, tvl_label)\n",
    "tvl_loader = torch.utils.data.DataLoader(dataset=tvl_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=vocab_collate_func, \n",
    "                                           shuffle=True)\n",
    "print(test_model(fic_loader, model3))\n",
    "print(test_model(fic_loader, model6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
